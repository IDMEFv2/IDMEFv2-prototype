input {
  kafka {
    bootstrap_servers => "${BOOTSTRAP_SERVER}"
    client_id         => "processing"
    group_id         => "processing"
    topics            => ["${TOPIC_COLLECTED}_ruled"]
    codec             => json
  }
}

filter {
  ruby {
    path => "/usr/share/logstash/config/scripts/analyzer.rb"
  }

  ruby {
    path          => "/usr/share/logstash/config/scripts/rules.rb"
    script_params => { "rules" => "/usr/share/logstash/config/idmef/*.yml" }
  }

}

filter {
  grok {
    match             => {
      "[Attachment][RawLog][Content][client][address]"      =>
        "^(?:%{IP:[Attachment][RawLog][Content][client][ip]}|%{HOSTNAME:[Attachment][RawLog][Content][client][hostname]})$"

      "[Attachment][RawLog][Content][target][address]" =>
        "^(?:%{IP:[Attachment][RawLog][Content][target][ip]}|%{HOSTNAME:[Attachment][RawLog][Content][target][hostname]})$"

      "[Attachment][RawLog][Content][host][address]"        =>
        "^(?:%{IP:[Attachment][RawLog][Content][host][ip]}|%{HOSTNAME:[Attachment][RawLog][Content][host][hostname]})$"

      "[Attachment][RawLog][Content][observer][address]"    =>
        "^(?:%{IP:[Attachment][RawLog][Content][observer][ip]}|%{HOSTNAME:[Attachment][RawLog][Content][observer][hostname]})$"

      "[Attachment][RawLog][Content][server][address]"      =>
        "^(?:%{IP:[Attachment][RawLog][Content][server][ip]}|%{HOSTNAME:[Attachment][RawLog][Content][server][hostname]})$"

      "[Attachment][RawLog][Content][source][address]"      =>
        "^(?:%{IP:[Attachment][RawLog][Content][source][ip]}|%{HOSTNAME:[Attachment][RawLog][Content][source][hostname]})$"
    }
    break_on_match    => false
    ecs_compatibility => "disabled"
  }

  ruby {
    path          => "/usr/share/logstash/config/scripts/iana.rb"
    script_params => {
      'protocols' => '/usr/share/logstash/modules/netflow/configuration/logstash/dictionaries/iana_protocol_numbers.yml'
      'services'  => {
        'tcp'  => '/usr/share/logstash/modules/netflow/configuration/logstash/dictionaries/iana_service_names_tcp.yml'
        'udp'  => '/usr/share/logstash/modules/netflow/configuration/logstash/dictionaries/iana_service_names_udp.yml'
        'sctp' => '/usr/share/logstash/modules/netflow/configuration/logstash/dictionaries/iana_service_names_sctp.yml'
        'dccp' => '/usr/share/logstash/modules/netflow/configuration/logstash/dictionaries/iana_service_names_dccp.yml'
      }
    }
  }

  mutate {
    replace => {
      "[Analyzer][IP]" => "${ANALYZER_IP}"
    }
  }
}

filter {
  if "alert" not in [tags] {
    drop {}
  }

  mutate {
    replace => {
      '[Version]' => '2.D.V02'
    }
  }

  if [@metadata][kafka][key] != "IDMEFv2" {
    ruby {
      path => "/usr/share/logstash/config/scripts/fill_alert.rb"
    }

    ruby {
      code => "
        event.set('[CreateTime]', event.get('@timestamp').to_s)

        # Convert Confidence to a number
        v = event.get('[Confidence]')
        event.set('[Confidence]', v.to_f) unless v.nil?

        # Convert hashes back into lists.
        [
          '[Category]', '[AltNames]', '[AltCategory]', '[Ref]', '[CorrelID]', '[AggrCondition]', '[PredID]', '[RelID]',
          '[Analyzer][Category]', '[Analyzer][Data]', '[Analyzer][Method]',
          '[Sensor]', '[Source]', '[Target]', '[Vector]',
        ].each do |k|
          v = event.get(k)
          if v.is_a?(Hash) then
            v = v.values
            event.set(k, v)
          end

          # If dealing with a list of hashes, look for sub-hashes and convert them to lists too.
          if v.is_a?(Array) then
            v.each_with_index do |e, idx|
              if e.is_a?(Hash)
                e.each do |sk, sv|
                  if sv.is_a?(Hash) then
                    sv = sv.values

                    # Convert Source(*).Port(*)/Target(*).Port(*) to a list of integers.
                    if ['[Source]', '[Target]'].include?(k) and sk == 'Port'
                      begin
                        sv.map! { |n| n.to_i }
                      rescue => e
                      end
                    end

                    event.set(k+'['+idx.to_s+']['+sk+']', sv)
                  end
                end
              end
            end
          end
        end

        unless event.get('[Analyzer][Category]')
          event.set('[Analyzer][Category]', ['LOG', 'ETL'])
        end

        unless event.get('[Analyzer][Data]')
          event.set('[Analyzer][Data]', ['Log'])
        end

        unless event.get('[Analyzer][Method]')
          event.set('[Analyzer][Method]', ['Signature', 'Monitor'])
        end

        # Merge the TI into the Source(*).
        ['IP', 'Email'].each do |ti_type|
          ti_data = (event.get('[Attachment][CTI][Content]') || {}).fetch(ti_type, {})
          (event.get('[Source]') || []).each_with_index do |source, idx|
            v = source[ti_type]
            unless v.nil?
              ti = ti_data[v]
              if ti
                idmefv2_ti = source['TI'] || []
                attachments = source['Attachment'] || []
                event.set('[Source]['+idx.to_s+'][TI]', idmefv2_ti | ti)
                event.set('[Source]['+idx.to_s+'][Attachment]', attachments | ['CTI'])
              end
            end
          end
        end
      "
    }
  }

  prune {
    whitelist_names => [
      # Top-level IDMEF attributes
      'Version',
      'ID',
      'Entity',
      'Category',
      'ext-Category',
      'Cause',
      'Description',
      'Status',
      'Priority',
      'Confidence',
      'Note',
      'CreateTime',
      'StartTime',
      'EndTime',
      'AltNames',
      'AltCategory',
      'Ref',
      'CorrelID',
      'AggrCondition',
      'PredID',
      'RelID',

      # IDMEF classes
      'Analyzer',
      'Sensor',
      'Source',
      'Target',
      'Vector',
      'Attachment'
    ]

    add_field       => {
      "[@metadata][validate]" => "${VALIDATE:yes}"
    }
  }

  if ![ID] {
    uuid {
      target => "ID"
    }
  }

  if ![Version] {
    mutate {
      replace => {
        '[Version]' => '2.D.V02'
      }
    }
  }

  if [@metadata][validate] != "no" {
    schema_check {
      schema_path      => "/usr/share/logstash/config/IDMEFv2-light.schema"
      refresh_interval => 300
      debug_output     => true
      tag_on_failure   => "_schemacheckfailure"
      add_field        => {
        "[@metadata][valid]" => "yes"
      }
    }
  }
  mutate {
    copy => {
      "[tags]" => "[@metadata][tags]"
    }
  }

  if ![Priority] {
    mutate {
      add_field => {
        'Priority' => 'Unknown'
      }
    }
  }

  # Convert Attachment objects back to lists
  # and encode their content into a JSON string. 
  ruby {
    init => "require 'logstash/json'"
    code => "
    ['Attachment'].each do |field|
      entries = []
      (event.get(field) || {}).each do |k, v|
        v['Name'] = k
        unless v['Content'].is_a? String or v['Content'].nil?
          v['Content'] = LogStash::Json.dump(v['Content'])
          v['Size'] = v['Content'].bytesize if field == 'Attachment'
        end
        entries.append(v)
      end
      event.set(field, entries)
    end
    "
  }

}

output {
  # FIXIT : do not insert invalid IDMEFv2
  if "_schemacheckfailure" in [tags] {
    stdout { codec => json }
  }
  kafka {
    bootstrap_servers => "${BOOTSTRAP_SERVER}"
    client_id         => "processing"
    topic_id          => "${TOPIC_PROCESSED}"
    codec             => json
  }
}
